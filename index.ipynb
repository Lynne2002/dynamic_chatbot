{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06773d30",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8471b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia #Dynamic API\n",
    "import random # To generate random responses\n",
    "import string # For removing punctuation - data preprocessing\n",
    "import numpy as np # Handling arrays\n",
    "import io # Provides Python’s main facilities for dealing with various types of I/O - text, binary and raw I/O\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Data encoding\n",
    "from sklearn.metrics.pairwise import cosine_similarity # Similarity-based response generation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer # For Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e411a5d",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8cfe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(user_query):\n",
    "    corpus = wikipedia.summary(user_query)\n",
    "    sentence_tokens = nltk.sent_tokenize(corpus)\n",
    "    word_tokens = nltk.word_tokenize(corpus)\n",
    "    return sentence_tokens, word_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a89ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence (AI) is intelligence—perceiving, synthesizing, and inferring information—demonstrated by machines, as opposed to intelligence displayed by non-human animals and humans. Example tasks in which this is done include speech recognition, computer vision, translation between (natural) languages, as well as other mappings of inputs.\\nAI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).\\nAs machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success and renewed funding. AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical-statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. General intelligence (the ability to solve an arbitrary problem) is among the field\\'s long-term goals. To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques – including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.\\nThe field was founded on the assumption that human intelligence \"can be so precisely described that a machine can be made to simulate it\".\\nThis raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity. Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards beneficial goals.\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.summary(\"Artificial Intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d658259",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49458f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56bdfc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemTokens(tokens):\n",
    "    lst = []\n",
    "    for i in tokens: #Every individual token has to be lemmatized\n",
    "        lst.append(lemmatizer.lemmatize(i))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5b61c",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "##### Noise removal of special characters - punctuation marks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2975b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d872d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = dict((ord(i), None) for i in string.punctuation)\n",
    "# ord -> the inbuilt function in python for returning Unicode value of a corresponding character.\n",
    "# Each value is replaced with \"None\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64af5dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: None,\n",
       " 34: None,\n",
       " 35: None,\n",
       " 36: None,\n",
       " 37: None,\n",
       " 38: None,\n",
       " 39: None,\n",
       " 40: None,\n",
       " 41: None,\n",
       " 42: None,\n",
       " 43: None,\n",
       " 44: None,\n",
       " 45: None,\n",
       " 46: None,\n",
       " 47: None,\n",
       " 58: None,\n",
       " 59: None,\n",
       " 60: None,\n",
       " 61: None,\n",
       " 62: None,\n",
       " 63: None,\n",
       " 64: None,\n",
       " 91: None,\n",
       " 92: None,\n",
       " 93: None,\n",
       " 94: None,\n",
       " 95: None,\n",
       " 96: None,\n",
       " 123: None,\n",
       " 124: None,\n",
       " 125: None,\n",
       " 126: None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5c2b86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca692d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization and lemmatization\n",
    "def lemmer(text):\n",
    "    tokenized_text = nltk.word_tokenize(text.lower().translate(punct)) #Tokenization of text and Removal of punctuation\n",
    "    lemmatized_values = lemTokens(tokenized_text) # Calling the lemTokens function\n",
    "    return lemmatized_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f2e713f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'have', 'a', 'headache']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test of lemmer function\n",
    "lemmer(\"I hav$$$$e a hea**dache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8815a",
   "metadata": {},
   "source": [
    "# Greeting - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df59833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_inputs = ['hi','hello','hey', 'greetings']\n",
    "greeting_responses = ['Hi, My name is ChatBot. How may I help you?', 'Hey, My name is ChatBot. How may I help you?','Hello, My name is ChatBot. How may I help you?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25120837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greeting(text):\n",
    "    for token in text.split():\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8108c63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey, My name is ChatBot. How may I help you?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the greeting function\n",
    "greeting(\"Hello I need some help in JavaScript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93312325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitation - If we type an input \"Hello,\" like a greeting then followed comma, the chatbot won't recognize it as a greeting\n",
    "greeting(\"Hello, I need some help in Javacript\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafed5f0",
   "metadata": {},
   "source": [
    "# Responses - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0f119e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(user_query):\n",
    "    bot_response = ''\n",
    "\n",
    "    # Tokenize the user query\n",
    "    sent_tokens, word_tokens = tokenize(user_query)\n",
    "    sent_tokens.append(user_query)\n",
    "\n",
    "    # Data encoding -> Converting sent_tokens to vectors\n",
    "    tfidf_obj = TfidfVectorizer(tokenizer = lemmer, stop_words = \"english\" )\n",
    "    tfidf = tfidf_obj.fit_transform(sent_tokens)\n",
    "\n",
    "\n",
    "    # Cosine similarity\n",
    "    # In this case tfidf = [t1, 12, t3, t4, user_query]\n",
    "    # Get the last element (user_query) which is in index [-1] and compare it with the entire list (t1, t2, t3, t4....)\n",
    "\n",
    "    sim_values = cosine_similarity(tfidf[-1],tfidf) # Cosine similarity of the last element with the entire list\n",
    "\n",
    "    # Selecting the token with masimum similarity value\n",
    "    # -2 means the second last response -> because the last response ([-1]) is the user query\n",
    "    index = sim_values.argsort()[0][-2] # Sorting values to give the index\n",
    "\n",
    "\n",
    "    flattened_sim = sim_values.flatten() #Flattening the sim_values to make them one dimensional\n",
    "    flattened_sim.sort() # Sorting flattened sim_values\n",
    "\n",
    "    required_tfidf = flattened_sim[-2]\n",
    "\n",
    "    if(required_tfidf ==0):\n",
    "        bot_response += \"Sorry, I cannot understand your question. I can only respond to questions in my corpus\"\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response += bot_response + sent_tokens[index]\n",
    "        return bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f3fa620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATBOT\n",
      "Hi\n",
      "Bot: Hello, My name is ChatBot. How may I help you?\n",
      "Who is Smith Wigglesworth?\n",
      "Chatbot:  Smith Wigglesworth (10 June 1859 – 12 March 1947) was a British evangelist who was influential in the early history of Pentecostalism.\n",
      "What is an apple tree?\n",
      "Chatbot:  An apple is an edible fruit produced by an apple tree (Malus domestica).\n",
      "What is a Turing machine?\n",
      "Chatbot:  The Turing machine was invented in 1936 by Alan Turing, who called it an \"a-machine\" (automatic machine).\n",
      "What is Artificial Intelligence?\n",
      "Chatbot:  Artificial intelligence art is any artwork created through the use of artificial intelligence (AI) programs such as text-to-image models and musical generators.\n",
      "exit\n",
      "ChatBot : Bye! I'm glad to be of assistance to you:)\n"
     ]
    }
   ],
   "source": [
    "print('CHATBOT')\n",
    "flag = 1\n",
    "\n",
    "while (flag == 1):\n",
    "    user_query = input()\n",
    "    user_query = user_query.lower()\n",
    "\n",
    "    # If user wants to exit\n",
    "    if(user_query == 'exit'):\n",
    "        flag = 0\n",
    "        print('ChatBot : Bye! I\\'m glad to be of assistance to you:)')\n",
    "\n",
    "    else:\n",
    "        # If user enters a greeting\n",
    "        if(greeting(user_query)!= None):\n",
    "            print(\"Bot: \" +greeting(user_query))\n",
    "        else:\n",
    "            res = respond(user_query)\n",
    "            print(\"Chatbot: \", res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
